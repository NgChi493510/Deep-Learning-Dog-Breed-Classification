{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":85201,"databundleVersionId":9605463,"sourceType":"competition"}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-20T16:15:17.356147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model VGG16","metadata":{}},{"cell_type":"code","source":"import os\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport torch\nimport pandas as pd\nfrom torchvision import transforms\nfrom torch.utils.data import random_split, DataLoader\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.optim as optim","metadata":{"execution":{"iopub.status.busy":"2024-09-21T01:19:08.918831Z","iopub.execute_input":"2024-09-21T01:19:08.919213Z","iopub.status.idle":"2024-09-21T01:19:08.924752Z","shell.execute_reply.started":"2024-09-21T01:19:08.919175Z","shell.execute_reply":"2024-09-21T01:19:08.923629Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"class ImageDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None, is_test=False):\n        self.data = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n        self.is_test = is_test\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        if not self.is_test:\n            # Fetch training images using paths from train.csv\n            img_path = os.path.join(self.root_dir, self.data.iloc[idx, 1])\n            image = Image.open(img_path).convert('RGB')\n            label = int(self.data.iloc[idx, 0].split('_')[1]) - 1  # Labeling\n            label = torch.tensor(label)\n            if self.transform:\n                image = self.transform(image)\n            return image, label\n        else:\n            # For test data\n            img_path = os.path.join(self.root_dir, self.data.iloc[idx, 1])\n            image = Image.open(img_path).convert('RGB')\n            if self.transform:\n                image = self.transform(image)\n            return image, self.data.iloc[idx, 0]","metadata":{"execution":{"iopub.status.busy":"2024-09-21T01:42:33.654404Z","iopub.execute_input":"2024-09-21T01:42:33.655179Z","iopub.status.idle":"2024-09-21T01:42:33.664280Z","shell.execute_reply.started":"2024-09-21T01:42:33.655138Z","shell.execute_reply":"2024-09-21T01:42:33.663352Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n# Create datasets\ntrain_dataset = ImageDataset(\n    csv_file='/kaggle/input/dl-63-cw-image-classification/train.csv',  # Correct path to train.csv\n    root_dir='/kaggle/input/dl-63-cw-image-classification/train',  # Directory containing images\n    transform=transform\n)\n\ntest_dataset = ImageDataset(\n    csv_file='/kaggle/input/dl-63-cw-image-classification/test.csv',  # Correct path to test.csv\n    root_dir='/kaggle/input/dl-63-cw-image-classification/test/',  # Directory containing test images\n    transform=transform, is_test=True\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T01:42:36.342745Z","iopub.execute_input":"2024-09-21T01:42:36.343423Z","iopub.status.idle":"2024-09-21T01:42:36.362675Z","shell.execute_reply.started":"2024-09-21T01:42:36.343381Z","shell.execute_reply":"2024-09-21T01:42:36.361953Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"train_size = int(0.8 * len(train_dataset))\nvalid_size = len(train_dataset) - train_size\n\ntrainset, validset = random_split(train_dataset, [train_size, valid_size])\n\n# Create DataLoaders\nbatch_size = 32\ntrain_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(validset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T01:42:40.197993Z","iopub.execute_input":"2024-09-21T01:42:40.198668Z","iopub.status.idle":"2024-09-21T01:42:40.206269Z","shell.execute_reply.started":"2024-09-21T01:42:40.198626Z","shell.execute_reply":"2024-09-21T01:42:40.205322Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"# class TestDataset(Dataset):\n#     def __init__(self, img_paths, transform=None):\n#         self.img_paths = img_paths\n#         self.transform = transform\n\n#     def __len__(self):\n#         return len(self.img_paths)\n\n#     def __getitem__(self, idx):\n#         img_path = self.img_paths[idx]\n#         image = Image.open(img_path).convert('RGB')\n\n#         if self.transform:\n#             image = self.transform(image)\n\n#         return image\n\n# # Load test data\n# test_df = pd.read_csv('/kaggle/input/dl-63-cw-image-classification/test.csv')\n# image_paths = test_df['filepaths'].values\n# image_ids = test_df['ID'].values\n# base_path = '/kaggle/input/dl-63-cw-image-classification/test/'\n# image_paths = [os.path.join(base_path, path) for path in image_paths]\n\n# testset = TestDataset(img_paths=image_paths, transform=transform)\n# testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-20T23:52:39.050792Z","iopub.execute_input":"2024-09-20T23:52:39.051186Z","iopub.status.idle":"2024-09-20T23:52:39.068643Z","shell.execute_reply.started":"2024-09-20T23:52:39.051139Z","shell.execute_reply":"2024-09-20T23:52:39.067858Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class CustomVGG16(nn.Module):\n    def __init__(self, num_classes=71):  # Ensure correct number of classes\n        super(CustomVGG16, self).__init__()\n\n        # Load pre-trained VGG16 model\n        self.base_model = models.vgg16(pretrained=True)\n\n        # Freeze the feature extraction layers\n        for param in self.base_model.features.parameters():  # Freeze only features, not classifier\n            param.requires_grad = False\n\n        # Modify the classifier part of the model\n        self.base_model.classifier = nn.Sequential(\n            nn.Linear(25088, 256),  # Input size from VGG16 architecture\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(256, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.base_model.features(x)  # Feature extraction\n        x = self.base_model.avgpool(x)   # Avg pooling\n        x = torch.flatten(x, 1)          # Flatten\n        x = self.base_model.classifier(x)  # Classifier\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-09-21T01:19:44.889240Z","iopub.execute_input":"2024-09-21T01:19:44.889973Z","iopub.status.idle":"2024-09-21T01:19:44.897989Z","shell.execute_reply.started":"2024-09-21T01:19:44.889935Z","shell.execute_reply":"2024-09-21T01:19:44.897081Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = CustomVGG16(num_classes=71).to(device)\noptimizer = optim.RMSprop(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\ndef train_and_val_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=25):\n    best_val_acc = 0.0\n    for epoch in range(num_epochs):\n        model.train()  # Set to training mode\n        running_loss, running_corrects = 0.0, 0\n\n        # Training loop\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            _, preds = torch.max(outputs, 1)\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n        print(f'Training - Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n\n        # Validation loop\n        model.eval()  # Set to evaluation mode\n        val_corrects = 0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n                val_corrects += torch.sum(preds == labels.data)\n\n        val_acc = val_corrects.double() / len(val_loader.dataset)\n        print(f'Validation Accuracy: {val_acc:.4f}')\n\n        # Save the best model based on validation accuracy\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), 'best_model.pth')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-21T01:19:48.307175Z","iopub.execute_input":"2024-09-21T01:19:48.307789Z","iopub.status.idle":"2024-09-21T01:19:49.855520Z","shell.execute_reply.started":"2024-09-21T01:19:48.307751Z","shell.execute_reply":"2024-09-21T01:19:49.854548Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\n","output_type":"stream"}]},{"cell_type":"code","source":"train_and_val_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T01:20:56.230920Z","iopub.execute_input":"2024-09-21T01:20:56.231339Z","iopub.status.idle":"2024-09-21T01:29:25.475553Z","shell.execute_reply.started":"2024-09-21T01:20:56.231301Z","shell.execute_reply":"2024-09-21T01:29:25.474553Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"Training - Epoch 1/10, Loss: 4.2668, Accuracy: 0.2391\nValidation Accuracy: 0.5742\nTraining - Epoch 2/10, Loss: 2.1124, Accuracy: 0.4520\nValidation Accuracy: 0.7208\nTraining - Epoch 3/10, Loss: 1.6236, Accuracy: 0.5615\nValidation Accuracy: 0.7541\nTraining - Epoch 4/10, Loss: 1.3063, Accuracy: 0.6430\nValidation Accuracy: 0.7830\nTraining - Epoch 5/10, Loss: 1.1402, Accuracy: 0.6825\nValidation Accuracy: 0.7836\nTraining - Epoch 6/10, Loss: 1.0280, Accuracy: 0.7122\nValidation Accuracy: 0.8006\nTraining - Epoch 7/10, Loss: 0.9986, Accuracy: 0.7346\nValidation Accuracy: 0.8233\nTraining - Epoch 8/10, Loss: 0.8942, Accuracy: 0.7576\nValidation Accuracy: 0.8082\nTraining - Epoch 9/10, Loss: 0.8404, Accuracy: 0.7786\nValidation Accuracy: 0.8220\nTraining - Epoch 10/10, Loss: 0.7864, Accuracy: 0.7881\nValidation Accuracy: 0.8226\n","output_type":"stream"}]},{"cell_type":"code","source":"# Unfreeze the last few layers for fine-tuning\nfor param in model.base_model.features[15:].parameters():\n    param.requires_grad = True\n\n# Re-define optimizer for fine-tuning\noptimizer = optim.SGD(model.parameters(), lr=0.001)\n\n# Fine-tune the model\ntrain_and_val_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T01:29:29.625117Z","iopub.execute_input":"2024-09-21T01:29:29.625499Z","iopub.status.idle":"2024-09-21T01:40:05.445144Z","shell.execute_reply.started":"2024-09-21T01:29:29.625462Z","shell.execute_reply":"2024-09-21T01:40:05.444193Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Training - Epoch 1/10, Loss: 0.5686, Accuracy: 0.8320\nValidation Accuracy: 0.8472\nTraining - Epoch 2/10, Loss: 0.5264, Accuracy: 0.8309\nValidation Accuracy: 0.8484\nTraining - Epoch 3/10, Loss: 0.4329, Accuracy: 0.8614\nValidation Accuracy: 0.8648\nTraining - Epoch 4/10, Loss: 0.4287, Accuracy: 0.8567\nValidation Accuracy: 0.8560\nTraining - Epoch 5/10, Loss: 0.4017, Accuracy: 0.8715\nValidation Accuracy: 0.8635\nTraining - Epoch 6/10, Loss: 0.3914, Accuracy: 0.8693\nValidation Accuracy: 0.8629\nTraining - Epoch 7/10, Loss: 0.3582, Accuracy: 0.8790\nValidation Accuracy: 0.8698\nTraining - Epoch 8/10, Loss: 0.3769, Accuracy: 0.8752\nValidation Accuracy: 0.8660\nTraining - Epoch 9/10, Loss: 0.3473, Accuracy: 0.8817\nValidation Accuracy: 0.8673\nTraining - Epoch 10/10, Loss: 0.3533, Accuracy: 0.8817\nValidation Accuracy: 0.8723\n","output_type":"stream"}]},{"cell_type":"code","source":"# model.eval()\n# test_loss, correct_test = 0.0, 0\n# with torch.no_grad():\n#     for images, labels in test_loader:\n#         images, labels = images.cuda(), labels.cuda()\n#         outputs = model(images)\n#         loss = criterion(outputs, labels)\n#         test_loss += loss.item() * images.size(0)\n#         _, predicted = torch.max(outputs.data, 1)\n#         correct_test += (predicted == labels).sum().item()\n\n# test_loss /= len(test_loader.dataset)\n# test_accuracy = correct_test / len(test_loader.dataset)\n# print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_model_on_test_set(model, test_loader, device, class_mapping):\n    model.eval()\n    predictions = []\n    ids = []\n    \n    with torch.no_grad():\n        for images, image_ids in test_loader:\n            images = images.to(device)\n            outputs = model(images)\n            _, predicted_classes = torch.max(outputs, 1)\n            \n            # Chuyển predicted_classes từ số thành tên class\n            predicted_classes = [class_mapping[p.item()] for p in predicted_classes]\n            \n            # Lưu lại image_ids từ test_loader thay vì sử dụng tensor index\n            predictions.extend(predicted_classes)\n            ids.extend(image_ids)  # image_ids chính là các ID từ file test.csv\n    \n    return ids, predictions\n\n# Map số index về class_name\nclass_mapping = {i: f'class_{i+1}' for i in range(71)}","metadata":{"execution":{"iopub.status.busy":"2024-09-21T01:56:51.740096Z","iopub.execute_input":"2024-09-21T01:56:51.740465Z","iopub.status.idle":"2024-09-21T01:56:51.747851Z","shell.execute_reply.started":"2024-09-21T01:56:51.740432Z","shell.execute_reply":"2024-09-21T01:56:51.746853Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"ids, predictions = evaluate_model_on_test_set(model, test_loader, device, class_mapping)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T01:56:54.124574Z","iopub.execute_input":"2024-09-21T01:56:54.125423Z","iopub.status.idle":"2024-09-21T01:57:03.268729Z","shell.execute_reply.started":"2024-09-21T01:56:54.125379Z","shell.execute_reply":"2024-09-21T01:57:03.267850Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"renamed_ids = list(range(len(ids)))","metadata":{"execution":{"iopub.status.busy":"2024-09-21T01:59:07.915442Z","iopub.execute_input":"2024-09-21T01:59:07.915819Z","iopub.status.idle":"2024-09-21T01:59:07.920787Z","shell.execute_reply.started":"2024-09-21T01:59:07.915782Z","shell.execute_reply":"2024-09-21T01:59:07.919786Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"results = pd.DataFrame({\n    'ID': renamed_ids, \n    'TARGET': predictions \n})\n\n# Lưu kết quả thành file CSV\nresults.to_csv('submission1.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-09-21T01:59:27.127229Z","iopub.execute_input":"2024-09-21T01:59:27.127986Z","iopub.status.idle":"2024-09-21T01:59:27.136823Z","shell.execute_reply.started":"2024-09-21T01:59:27.127947Z","shell.execute_reply":"2024-09-21T01:59:27.135982Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"results","metadata":{"execution":{"iopub.status.busy":"2024-09-21T01:59:29.958055Z","iopub.execute_input":"2024-09-21T01:59:29.958461Z","iopub.status.idle":"2024-09-21T01:59:29.969265Z","shell.execute_reply.started":"2024-09-21T01:59:29.958419Z","shell.execute_reply":"2024-09-21T01:59:29.968221Z"},"trusted":true},"execution_count":72,"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"        ID    TARGET\n0        0  class_58\n1        1  class_58\n2        2   class_7\n3        3  class_17\n4        4  class_71\n...    ...       ...\n1395  1395  class_69\n1396  1396  class_51\n1397  1397  class_65\n1398  1398  class_46\n1399  1399  class_55\n\n[1400 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>TARGET</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>class_58</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>class_58</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>class_7</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>class_17</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>class_71</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1395</th>\n      <td>1395</td>\n      <td>class_69</td>\n    </tr>\n    <tr>\n      <th>1396</th>\n      <td>1396</td>\n      <td>class_51</td>\n    </tr>\n    <tr>\n      <th>1397</th>\n      <td>1397</td>\n      <td>class_65</td>\n    </tr>\n    <tr>\n      <th>1398</th>\n      <td>1398</td>\n      <td>class_46</td>\n    </tr>\n    <tr>\n      <th>1399</th>\n      <td>1399</td>\n      <td>class_55</td>\n    </tr>\n  </tbody>\n</table>\n<p>1400 rows × 2 columns</p>\n</div>"},"metadata":{}}]}]}