{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport torch\nimport torch.nn.functional as F\nimport pandas as pd\nfrom torchvision import transforms\nfrom torch.utils.data import random_split, DataLoader\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.optim as optim","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImageDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None, is_test=False):\n        self.data = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n        self.is_test = is_test\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        if not self.is_test:\n            # Fetch training images using paths from train.csv\n            img_path = os.path.join(self.root_dir, self.data.iloc[idx, 1])\n            image = Image.open(img_path).convert('RGB')\n            label = int(self.data.iloc[idx, 0].split('_')[1]) - 1  # Labeling\n            label = torch.tensor(label)\n            if self.transform:\n                image = self.transform(image)\n            return image, label\n        else:\n            # For test data\n            img_path = os.path.join(self.root_dir, self.data.iloc[idx, 1])\n            image = Image.open(img_path).convert('RGB')\n            if self.transform:\n                image = self.transform(image)\n            return image, self.data.iloc[idx, 0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n])\n\n\n# Create datasets\ntrain_dataset = ImageDataset(\n    csv_file='/kaggle/input/dl-63-cw-image-classification/train.csv',  # Correct path to train.csv\n    root_dir='/kaggle/input/dl-63-cw-image-classification/train',  # Directory containing images\n    transform=transform\n)\n\ntest_dataset = ImageDataset(\n    csv_file='/kaggle/input/dl-63-cw-image-classification/test.csv',  # Correct path to test.csv\n    root_dir='/kaggle/input/dl-63-cw-image-classification/test/',  # Directory containing test images\n    transform=transform, is_test=True\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_size = int(0.8 * len(train_dataset))\nvalid_size = len(train_dataset) - train_size\n\ntrainset, validset = random_split(train_dataset, [train_size, valid_size])\n\n# Create DataLoaders\nbatch_size = 32\ntrain_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(validset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomConvNeXt(nn.Module):\n    def __init__(self, num_classes=71):\n        super(CustomConvNeXt, self).__init__()\n        \n        # Load pre-trained ConvNeXt model (convnext_large or convnext_base)\n        self.base_model = models.convnext_large(pretrained=True)\n        \n        # Freeze earlier layers (feature extraction layers)\n        for param in self.base_model.features.parameters():\n            param.requires_grad = False  # Freeze feature extraction layers\n\n        # Replace the classifier layer with a new fully connected layer\n        self.base_model.classifier[2] = nn.Sequential(\n            nn.Linear(self.base_model.classifier[2].in_features, 256),  # in_features dựa trên ConvNeXt architecture\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(256, num_classes)  # Final output layer với num_classes outputs\n        )\n\n    def forward(self, x):\n        return self.base_model(x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Instantiate the model\nmodel = CustomConvNeXt(num_classes=71).to(device)\n\n# Optimizer and Loss Function\noptimizer = optim.RMSprop(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\ncriterion = nn.CrossEntropyLoss()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_and_val_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10):\n    best_val_acc = 0.0\n    for epoch in range(num_epochs):\n        model.train()  # Set to training mode\n        running_loss, running_corrects = 0.0, 0\n\n        # Training loop\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            _, preds = torch.max(outputs, 1)\n            running_loss += loss.item() * inputs.size(0)\n            running_corrects += torch.sum(preds == labels.data)\n\n        epoch_loss = running_loss / len(train_loader.dataset)\n        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n        print(f'Training - Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n\n        # Validation loop\n        model.eval()  # Set to evaluation mode\n        val_corrects = 0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n                val_corrects += torch.sum(preds == labels.data)\n\n        val_acc = val_corrects.double() / len(val_loader.dataset)\n        print(f'Validation Accuracy: {val_acc:.4f}')\n\n        # Save the best model based on validation accuracy\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), 'best_convnext_model.pth')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_and_val_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=10)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for param in model.base_model.features[-8:].parameters():\n    param.requires_grad = True\n\n# Re-define optimizer with a lower learning rate\noptimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4, momentum=0.9)\n\n# Learning rate scheduler\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n\n# Fine-tune the model for additional epochs\ntrain_and_val_model(model, criterion, optimizer, train_loader, val_loader, num_epochs=5)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_model_on_test_set(model, test_loader, device, class_mapping):\n    model.eval()\n    predictions = []\n    ids = []\n    \n    with torch.no_grad():\n        for images, image_ids in test_loader:\n            images = images.to(device)\n            outputs = model(images)\n            _, predicted_classes = torch.max(outputs, 1)\n            \n            predicted_classes = [class_mapping[p.item()] for p in predicted_classes]\n            \n        \n            predictions.extend(predicted_classes)\n            ids.extend(image_ids)  # image_ids chính là các ID từ file test.csv\n    \n    return ids, predictions\n\nclass_mapping = {i: f'class_{i+1}' for i in range(71)}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ids, predictions = evaluate_model_on_test_set(model, test_loader, device, class_mapping)\nrenamed_ids = list(range(len(ids)))\nresults = pd.DataFrame({\n    'ID': renamed_ids, \n    'TARGET': predictions \n})\n\nresults.to_csv('submission34.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}